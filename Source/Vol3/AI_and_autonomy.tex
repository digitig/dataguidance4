%================================================================================
%       Safety Critical Systems Club - Data Safety Initiative Working Group
%================================================================================
%                       DDDD    SSSS  IIIII  W   W   GGGG
%                       D   D  S        I    W   W  G   
%                       D   D   SSS     I    W W W  G  GG
%                       D   D      S    I    WW WW  G   G
%                       DDDD   SSSS   IIIII  W   W   GGG
%================================================================================
%               Data Safety Guidance Document - LaTeX Source File
%================================================================================
%
% Description:
%   AI and autonomy section.
%
%================================================================================
\chapter{AI and autonomy (Informative)} \label{bkm:autonomy}\index{AI|textbf}\index{Automation|textbf}
%
%\dsiwgSectionQuote{The first rule of any technology used in a business is that automation applied to an efficient operation will magnify the efficiency.
%  The second is that automation applied to an inefficient operation will magnify the inefficiency.}{Bill Gates}\todo{Pick a suitable quote}
%
\dsiwgSectionQuote{The potential benefits of artificial intelligence are huge, so are the dangers.}{Dave Waters}
%
%
%
\section{Job Displacement}\index{AI!Job displacement}
\Gls{ai} and automation\index{Automation} can lead to the displacement of jobs, as machines can perform some tasks more efficiently than humans. This can have significant socioeconomic implications, including increased unemployment\index{Unemployment} and wage stagnation in affected industries. The \gls{imf}, in January 2024, gave an estimate that \gls{ai} will impact 40\% of jobs globally, and 60\% of highly skilled work, (\cite{citation:Cazziniga2024}). The report balances the already known complementary nature of \gls{ai} to human work against the likely detrimental aspects. Unusually for technological advances, \gls{ai} is expected to impact high-skilled jobs to a greater extent than manual skills. The report states, ``advanced economies face greater risks from \gls{ai} -- but also more opportunities to leverage its benefits''. Labour requirements could significantly reduce, wages could be lower and of course jobs will disappear.  Managing director of the \gls{imf}, Kristalina Georgieva, said that ``\gls{ai} will likely worsen overall inequality , a troubling trend that policymakers must proactively address to prevent the technology from further stoking social tensions.'' (\cite{citation:toh2024}). How economists are changing the standard growth view of economics and how engineering and other businesses can engage with such  developing problems is discussed in an article in the February 2024 edition of the \gls{scsc} Newsletter (\cite{citation:SCSC193}). The data used to determine the impact of an \gls{ai} system, especially when it affects currently underdeveloped nations, must ensure that the social tension Georgieva refers to does not reach a breaking point. Like well digging, rail construction and careful dam placement, \gls{ai} technology could raise many millions out of poverty if it is applied to benefit life on earth.

\section{Bias and Discrimination}\index{AI!Bias}\index{AI!Discrimination}
\gls{ai} systems can inherit biases present in their training\index{Training!AI} data, leading to discriminatory practices. For instance, facial recognition software has been shown to have higher error rates for people of certain racial and ethnic groups. At a very basic level, slight imperfections in coatings for car registration plates, easily ignored by humans, has made some number plates unrecognisable to automatic number plate recognition cameras at car parks. This can be easily missed in training\index{Training!AI} the system since it would have been difficult to anticipate every possible imperfect state of a number plate. In this case advances are made from experience but there are many situations in which day one of a system’s deployment has to be near perfect. Training data sets should be vetted by committees, as safety usually is and should be always. That technique is more likely to bring out overlooked areas, obvious to someone on the committee but totally missed by others. Testing of developed systems should also cover the full range of possibilities, whether or not those involved in the development provide assurances, as assurances can hide failings.

\section{Loss of Human Skills}\index{Autonomy!Loss of human skills}
With \gls{ai} taking over tasks such as navigation or memory-dependent activities, there is a risk that humans may lose certain skills that are not used regularly, potentially reducing cognitive abilities over time.  However, the opposite is also true because, driven by the threat of automation\index{Automation}, many people could turn to artisan skills to create unique or less ubiquitous products providing an element of appealing individuality. This could turn supermarket checkout staff into, for instance, entrepreneurial upholsterers or cosmetic developers when and where all checkouts have become automated. Regardless of whether or not skills are lost, it is certain that in order to verify and validate data intensive artificial intelligence systems someone will be required to maintain those basic skills to provide the assurance. To make a simple analogy, one cannot validate a French grammar correcting \gls{ai} system if one does not speak French fluently.

\section{Security Risks}\index{AI!Security risks}
\gls{ai} can be used to develop sophisticated cyber-attacks, and \gls{ai} systems themselves can be vulnerable to such attacks. The integration of \gls{ai} into critical infrastructure heightens the potential impact of these risks. The issue of autonomous vehicles in military applications has been discussed in an SCSC newsletter, (\cite{citation:SCSC152}), which quotes ``Formal Verification of Ethical Choices in an Autonomous System'' (\cite{citation:Dennis2015}): ``All participants in society are required to follow specific regulations and laws. An autonomous system cannot be an exception''. Given this as an accepted  principle, it is then incumbent on engineers to design systems which follow ethical regulations. This presents difficulties and it demands that the most sophisticated sensors be employed on killer robots when decisions will need to be made as to whether or not to engage in an action which may result in death or excessive damage. Such examples are, firing a missile at a market square or bridge where known enemies are observed. At what point does the death of others not engaged in wrong doing become acceptable, known as collateral damage. It is important to stress that  the data used to train systems covers all possibilities, just as in human vs.  human conflict. If the machine is not programmed to make the decision, the machine must request clarification from human controllers. The issue is that it will almost certainly prove impossible to develop military killer systems that can be programmed with sufficient data to make every decision required in all circumstances without needing to consult humans, though direct accurate hits on isolated targets with little risk to the combatants of the targeting force is likely to remain an acceptable usage. But it would be sad if the governments of the world end up pitting machines they don’t understand against each other with all the horrendous possible consequences.

\section{Privacy Erosion}\index{AI!Privacy erosion}
\gls{ai}'s ability to analyse vast quantities of personal data can lead to erosion of privacy. For instance, \gls{ai} can be used to make highly accurate predictions about individuals’ behaviours, preferences, and even future actions. To a limited extent this is already operational and desired by police forces wanting to identify faces and even the gait of wanted persons. The usual claim is that ``If you are not doing anything wrong you have nothing to fear''. But many democracies are turning against the principle of eroding privacy, considering some policing techniques \gls{ai} makes available are too intrusive. Even when the nation is particularly controlling, the creation of the network of cameras across a nation does provide the environment for a takeover by controlling elements. The Metropolitan Police\index{Metropolitan Police}  of London are using \gls{lfr} cameras (\cite{citation:police2025}). Their policy is ``\Gls{lfr} cameras are focused on a specific area; when people pass through that area their images are streamed directly to the \glsxtrlong{lfr} system. This system contains a watchlist: a list of offenders wanted by the police and/or the courts, or those who pose a risk of harm to themselves or others.''  They currently assert that ``\gls{lfr} is not a ubiquitous tool that uses lots of \gls{cctv} cameras from across London to track every person’s movements''.  But a change of national or local government could change that. The risk is that data may be abused or false.

\section{Control, Autonomy, Ethical and Moral Considerations}\index{AI!Control, Autonomy, Ethical and Moral Considerations}
As \gls{ai} systems become more autonomous, there is a risk that they may act in unforeseen ways that are not aligned with human intentions or may be manipulated to act against human interests. Also there are significant ethical questions around \gls{ai}, including the morality of decisions made by \gls{ai} systems, particularly in life-and-death situations such as in autonomous vehicles or military applications. Although mainly concerned with military questions  “The Troubling Aspects of Autonomy” (\cite{citation:SCSC152}) discusses the issues of control that arise for dangerous life-threatening situations and is pertinent to non-military systems.

\section{Economic Inequality}\index{AI!Economic inequality}
The benefits of \gls{ai} may accrue disproportionately to those who own the technology, potentially exacerbating economic inequality. Companies and nations that can invest in \gls{ai} could gain significant economic advantages, leaving others behind. The likely beneficiaries are the developed economies. 40\% globally will be affected according to an \gls{imf} discussion paper and 60\% in the advanced technology world. In the fast world of competitive developments in \gls{ai} which has now begun,  it is difficult to believe that the potential pitfalls are ever fully examined when, just as in more conventional engineering systems, the bottom line remains money gained through business success. Only regulation by governments and other authorities can be expected to be effective in limiting damage outside the bounds of \gls{ai} systems. Some are now advocating a new form of economics known as Doughnut Economics as advocated by Professor Kate Raworth, senior associate at Oxford University’s Environmental Change Institute in her book on the subject (\cite{citationRaworth2017}). A brief discussion of the some of the issues of such economics can be found in (\cite{citation:SCSC193}). There could be ways to support business in developing countries if \gls{ai} tools are made available on smart phones at low cost to, for instance, African women’s business collectives. Such a system may just have emerged with China's DeepSeek tool. Data input and received from such applications will need to be safe and accurate if it is to benefit the less well-off of the world. Businesses must have good data to when using \gls{ai} that provides reasonably accurate appraisals of outcomes from various actions. Ultimately, nobody should be left falling short of the social foundation that the doughnut economy declares the future ‘bottom line’ to be. In the appendix to Raworth’s book, a table is given showing the issues that need addressing and declaring the sources. Such sources should be consulted for trusted data when new projects are developed. 

\section{Dependency}\index{AI!Dependency}
Over-reliance on \gls{ai} can lead to a lack of preparedness when systems fail. If critical infrastructure or services are \gls{ai}-dependent, outages or malfunctions could have severe consequences. Simple growth of single point failure understanding applied to \gls{ai} should mitigate a lot of the risk of loss of urgent data, such as fly-by-wire aircraft having three computers with a voting sub-system in overall control.

\section{Manipulation and Fake Content}\index{AI!Manipulation}\index{AI!Fake content}
\Gls{ai} can be used to create deep-fakes and synthetic media, which can be used to manipulate public opinion, perpetrate fraud, or spread misinformation. The \gls{pai}, (\cite{citation:partnershiponai}),  has pursued the concept of there being a spectrum of harm from manipulated media. How this will all resolve is as yet unknown; the problem is in its relative infancy. With partners, a working definition arrived at is ``any image or video with content or context edited along the `cheap fake' to `deepfake' spectrum with the potential to mislead and cause harm'' (\cite{citation:deepfakes}). A diagram and explanation of the spectrum can be found at Data and Society’s website (\cite{citation:deepfakesspectrum}). Twelve principles for labelling manipulated data can be found at the \gls{pai} on \gls{ai} (\cite{citation:twelveprinciples}) . As to whether any of this is enough, only time will tell. Much is seen in the news of demands for content providers to do better at eliminating bad data, be it manipulated or such things as sites encouraging suicide. However, it is worth noting that The \gls{fhi}, a research group within the Oxford Martin School, has joined the \gls{pai} and that organisation has as members many of the known big players in the data intensive media field: Amazon, Apple, Google/DeepMind, Facebook, IBM and Microsoft. There goal is to formulate socially beneficial best practices for \gls{ai} development. How much their belief in their achievements coincides with politicians requirements we must wait and see. The \gls{pai}has produced a document ``PAI’s responsible Practices for Synthetic Media'', (\cite{citation:pais}), which argues the best way to overcome fake media is by the organisations not trying to deceive anybody to have codes of practice. They seek to advance ethical and responsible behaviour. Synthetic media, also known as generative media is defined as ``..visual, auditory or multimodal content that has been generated or modified, commonly by artificial intelligence.''

Legitimate use of such media may be, for example, entertainment, art, satire, education, and research. Furthermore, techniques can be used legitimately or harmfully; there is no barrier. One recommendation for technology and infrastructure creators is ``aim to disclose in a manner that mitigates speculation about content, strives towards resilience to manipulation or forgery, is accurately applied and also when necessary, communicates uncertainty without furthering speculation''. The guide goes on to recommend practices for `Creators' and `Distributors and Publishers'. Transparency and disclosure are the main thrusts. It therefore appears that the legal definition for consumer risk will mirror goods purchase: \dsiwgTextIT{caveat emptor}, let the buyer beware, but with safeguards provided by companies wishing to take ethical stances.

\section{Existential Risk}\index{AI!Existential risk}
In the long term, some theorists and scientists worry that the creation of super-intelligent \gls{ai} could pose an existential risk to humanity if it becomes uncontrollable or its goals are misaligned with human values. This particular aspect of data safety with \gls{ai} systems shows how important it is that the data used in \gls{ai} systems reflects humanity's values. The doughnut economic model uses, as the social foundation for humanity, twelve of the seventeen goals of the United Nations. Any \gls{ai} system that is not trained, where relevant, using data that will ultimately achieve those goals, is almost certainly not an ethical system designed to align with the established values of the global humanitarian consciousness. Also, many systems capable of destroying that have been granted autonomy\index{Automation}, such as  weapons, need to have been trained using the relevant data reflecting relevant guidelines (in the military case, on conflict rules). Issues related to the military problems are discussed in \cite{citation:SCSC152}.

\section{An AI's view of the risks}
We asked an \gls{ai} ``What sort of mistakes due to \gls{ai} can be hard to detect?.'' The \gls{ai} replied as follows:

\begin{aibox}
\Glsxtrlong{ai} (\glsxtrshort{ai}) systems, while powerful and increasingly integral to many aspects of modern life, are not infallible. There are several types of mistakes or errors in \gls{ai} systems that can be particularly hard to detect:

Subtle Data Bias: \gls{ai} algorithms are often only as good as the data they are trained on. If this data contains biases, the \gls{ai} can perpetuate or even amplify these biases. These biases can be subtle and deeply ingrained in the data, making them difficult to detect without thorough analysis.

Overfitting: This occurs when an \gls{ai} model is too closely tailored to the specifics of its training\index{Training!AI} data and fails to generalize well to new, unseen data. Detecting overfitting can be challenging because the model may perform exceptionally well on training\index{Training!AI} data but poorly in real-world applications.

Black Box Models: Many advanced \gls{ai} models, like deep neural networks, are often considered ``black boxes'' because their decision-making processes are not transparent or interpretable. This lack of transparency makes it difficult to understand why the \gls{ai} made a certain decision and, consequently, to identify when it makes mistakes.

Contextual Misinterpretation: \gls{ai} may not fully understand or consider the context in which data exists, leading to misinterpretations. These errors can be hard to detect because the \gls{ai}'s output may be logically consistent within the data it has, but contextually inappropriate.

Feedback Loops: In systems where \gls{ai} output influences future input data, feedback loops can occur. For instance, in content recommendation algorithms, the system can get trapped in a loop of reinforcing its own biases, making it hard to detect errors in content filtering or recommendation.

Anomalous Data or Edge Cases: \gls{ai} systems might struggle with data or situations that are significantly different from what they were trained on, known as edge cases. These anomalies can lead to errors that are hard to anticipate and detect because they occur outside the 'normal' operating parameters.

Errors in Complex Interactions: In systems where multiple \gls{ai} models interact, errors can arise from the complex interplay between these systems. These errors can be particularly challenging to detect and diagnose because they may not arise from any single model but from the interaction between models.

Evolution of Language and Social Norms: \gls{ai} systems trained on language data can become outdated as language and social norms evolve. Detecting when an \gls{ai} system is no longer accurately interpreting or generating language according to contemporary standards can be difficult.

Misalignment with Human Values: \gls{ai} might make decisions that are technically correct but misaligned with human ethics or values. These types of mistakes can be particularly hard to detect because they require nuanced understanding of human ethics, culture, and values.

To mitigate these issues, continuous monitoring, testing, diverse data sets, and incorporating explainability in \gls{ai} systems are crucial. Additionally, involving human oversight and interdisciplinary expertise can help in detecting and correcting such hard-to-detect mistakes.
\end{aibox}
